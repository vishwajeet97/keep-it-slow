<TimeLimit<CartPoleEnv<CartPole-v0>>>
epochs=10100
epsilon=0.100000
N=1000
M=256
num_train_iters=128
num_ep_update_value=2001
gamma=1.000000
layer_size=[64]
lr=0.010000
decision_interval=8
Updating Q
Avg timesteps per episode: 9.360000
Num eps: 1000
Experience length: 1000
Updating Q
Avg timesteps per episode: 9.394000
Num eps: 2000
Experience length: 2000
Updating Q
Avg timesteps per episode: 9.359000
Num eps: 3000
Experience length: 3000
Updating Q
Avg timesteps per episode: 9.411000
Num eps: 4000
Experience length: 4000
Updating Q
Avg timesteps per episode: 9.452000
Num eps: 5000
Experience length: 5000
Updating Q
Avg timesteps per episode: 9.429000
Num eps: 6000
Experience length: 6000
Updating Q
Avg timesteps per episode: 9.444000
Num eps: 7000
Experience length: 7000
Updating Q
Avg timesteps per episode: 9.673000
Num eps: 8000
Experience length: 8000
Updating Q
Avg timesteps per episode: 9.582000
Num eps: 9000
Experience length: 9000
<TimeLimit<CartPoleEnv<CartPole-v0>>>
epochs=10100
epsilon=0.100000
N=1000
M=256
num_train_iters=128
num_ep_update_value=2001
gamma=1.000000
layer_size=[64]
lr=0.010000
decision_interval=8
Updating Q
Avg timesteps per episode: 9.360000
Num eps: 1000
Experience length: 1000
Updating Q
Avg timesteps per episode: 9.425000
Num eps: 2000
Experience length: 2000
Updating Q
Avg timesteps per episode: 9.401000
Num eps: 3000
Experience length: 3000
Updating Q
Avg timesteps per episode: 9.446000
Num eps: 4000
Experience length: 4000
Updating Q
Avg timesteps per episode: 9.430000
Num eps: 5000
Experience length: 5000
Updating Q
Avg timesteps per episode: 9.426000
Num eps: 6000
Experience length: 6000
Updating Q
Avg timesteps per episode: 9.435000
Num eps: 7000
Experience length: 7000
Updating Q
Avg timesteps per episode: 9.436000
Num eps: 8000
Experience length: 8000
Updating Q
Avg timesteps per episode: 9.423000
Num eps: 9000
Experience length: 9000
Updating Q
Avg timesteps per episode: 9.455000
Num eps: 10000
Experience length: 10000
